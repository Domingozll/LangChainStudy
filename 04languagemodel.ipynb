{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 04 大语言模型\n",
    "大型语言模型（LLM）是LangChain的核心组件。LangChain不提供自己的LLM，而是提供了一个标准接口，用于与许多不同的LLM进行交互。\n",
    "https://python.langchain.com/docs/modules/model_io/models/llms/\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "#设置代理\n",
    "import os\n",
    "os.environ['http_proxy'] = 'http://127.0.0.1:10809'\n",
    "os.environ['https_proxy'] = 'http://127.0.0.1:10809'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-22T05:46:53.126655500Z",
     "start_time": "2023-08-22T05:46:53.109655900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "llm = OpenAI()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-22T03:24:30.462793200Z",
     "start_time": "2023-08-22T03:24:30.437794700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "使用 LLM 的最简单方法是可调用对象：传入字符串，获取字符串完成。\n",
    "```shell\n",
    "__call__: string in -> string out\n",
    "```\n",
    "这是他内部函数"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\n\\n两个人在一起聊天，一个问另一个：\"你最近怎么样？\"\\n\\n另一个回答：\"我想像一根螺丝钉一样，拧得越来越紧！\"'"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#可以直接调用\n",
    "llm(\"给我讲一个笑话\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-22T03:25:42.574590400Z",
     "start_time": "2023-08-22T03:25:39.759421Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "generate: 批量调用，输出更丰富\n",
    "\n",
    "generate 允许您使用字符串列表调用模型，从而返回比文本更完整的响应。\n",
    "此完整响应可能包括多个热门响应和其他特定于LLM提供程序的信息："
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "llm_result = llm.generate([\"给我讲个笑话\", \"给我讲个诗词\"]*15)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-22T03:27:08.844560900Z",
     "start_time": "2023-08-22T03:26:57.761361500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "30"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(llm_result.generations)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-22T03:27:14.518626100Z",
     "start_time": "2023-08-22T03:27:14.509626300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "[Generation(text='\\n\\n笑话：\\n\\n一个男孩走进一家商店，问老板：“你有什么新鲜的东西卖吗？”\\n\\n老板回答：“有，我有一只新鲜的鸡！”\\n\\n男孩说：“太好了，我要买。你知道怎么把它变成鸭子吗？”\\n\\n老板说：“不知道，为什么？”\\n\\n男孩说：“因为我想给它一个惊喜！”', generation_info={'finish_reason': 'stop', 'logprobs': None})]"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_result.generations[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-22T03:27:18.618856500Z",
     "start_time": "2023-08-22T03:27:18.596859900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "[Generation(text='\\n\\n《春晓》\\n\\n春眠不觉晓，\\n处处闻啼鸟。\\n夜来风雨声，\\n花落知多少。', generation_info={'finish_reason': 'stop', 'logprobs': None})]"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_result.generations[1]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-22T03:27:53.282793400Z",
     "start_time": "2023-08-22T03:27:53.232783300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "[Generation(text='\\n\\n一个猴子去拜访朋友，准备给他带礼物，但是他不知道该带什么，于是他就想：“现在是冬天，我可以带一件厚厚的毛衣给他！”于是他就带了毛衣去拜访朋友，朋友很开心，问猴子：“你怎么知道我需要一件毛衣呢？”猴子说：“因为这是猴子送人的冬衣！”', generation_info={'finish_reason': 'stop', 'logprobs': None})]"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_result.generations[2]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-22T03:28:04.128493300Z",
     "start_time": "2023-08-22T03:28:04.058869400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "[Generation(text='\\n\\n春晓\\n\\n孟浩然\\n\\n春眠不觉晓，\\n\\n处处闻啼鸟。\\n\\n夜来风雨声，\\n\\n花落知多少。', generation_info={'finish_reason': 'stop', 'logprobs': None})]"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_result.generations[3]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-22T03:28:08.461815Z",
     "start_time": "2023-08-22T03:28:08.441814800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "{'token_usage': {'total_tokens': 4236,\n  'prompt_tokens': 435,\n  'completion_tokens': 3801},\n 'model_name': 'text-davinci-003'}"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#您还可以访问返回的提供程序特定信息。此信息在提供商之间没有标准化。\n",
    "llm_result.llm_output"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-22T03:28:39.561663400Z",
     "start_time": "2023-08-22T03:28:39.544660200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.1 异步接口\n",
    "LangChain通过利用asyncio库为LLM提供异步支持。\n",
    "异步支持对于同时调用多个 LLM 特别有用，因为这些调用是网络绑定的。\n",
    "目前、 OpenAI PromptLayerOpenAI ChatOpenAI 、 Anthropic 和 Cohere 受支持，但对其他 LLM 的异步支持已在路线图上。\n",
    "您可以使用该方法 agenerate 异步调用 OpenAI LLM。\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import openai\n",
    "openai.proxy = os.getenv('https_proxy')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-22T05:47:06.538233200Z",
     "start_time": "2023-08-22T05:47:06.520234500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# 导入所需的模块\n",
    "import time  # 用于计时\n",
    "import asyncio  # 用于处理异步编程\n",
    "\n",
    "from langchain.llms import OpenAI  # 从langchain.llms库导入OpenAI类\n",
    "\n",
    "# 定义一个串行（同步）方式生成文本的函数\n",
    "def generate_serially():\n",
    "    llm = OpenAI(temperature=0.9)  # 创建OpenAI对象，并设置temperature参数为0.9\n",
    "    for _ in range(10):  # 循环10次\n",
    "        resp = llm.generate([\"Hello, how are you?\"])  # 调用generate方法生成文本\n",
    "        print(resp.generations[0][0].text)  # 打印生成的文本\n",
    "\n",
    "# 定义一个异步生成文本的函数\n",
    "async def async_generate(llm):\n",
    "    resp = await llm.agenerate([\"Hello, how are you?\"])  # 异步调用agenerate方法生成文本\n",
    "    print(resp.generations[0][0].text)  # 打印生成的文本\n",
    "\n",
    "# 定义一个并发（异步）方式生成文本的函数\n",
    "async def generate_concurrently():\n",
    "    llm = OpenAI(temperature=0.9)  # 创建OpenAI对象，并设置temperature参数为0.9\n",
    "    tasks = [async_generate(llm) for _ in range(10)]  # 创建10个异步任务\n",
    "    await asyncio.gather(*tasks)  # 使用asyncio.gather等待所有异步任务完成\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-22T05:47:10.118831500Z",
     "start_time": "2023-08-22T05:47:07.863159600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "I'm doing well, thank you. How about you?\n",
      "\n",
      "I'm doing well, thank you. How about you?\n",
      "\n",
      "\n",
      "I'm doing well, thank you. How about you?\n",
      "\n",
      "\n",
      "I'm doing well, how about you?\n",
      "\n",
      "\n",
      "I'm doing well, thank you. How about you?\n",
      "\n",
      "\n",
      "I'm doing good, thanks! How about you?\n",
      "\n",
      "\n",
      "I'm doing well, thank you for asking! How about yourself?\n",
      "\n",
      "\n",
      "I'm doing well, thank you. How about you?\n",
      "\n",
      "\n",
      "I'm doing well, thanks for asking! How about you?\n",
      "\n",
      "\n",
      "I'm doing well, thank you. How about you?\n",
      "\u001B[1mConcurrent executed in 2.14 seconds.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "# 记录当前时间点\n",
    "s = time.perf_counter()\n",
    "# 使用异步方式并发执行生成文本的任务\n",
    "# 如果在Jupyter以外运行此代码，使用 asyncio.run(generate_concurrently())\n",
    "await generate_concurrently()\n",
    "# 计算并发执行所花费的时间\n",
    "elapsed = time.perf_counter() - s\n",
    "print(\"\\033[1m\" + f\"Concurrent executed in {elapsed:0.2f} seconds.\" + \"\\033[0m\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-22T05:47:13.611147200Z",
     "start_time": "2023-08-22T05:47:11.465602300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I'm doing well, thank you. How about you?\n",
      "\n",
      "\n",
      "I'm doing well, thank you. How about you?\n",
      "\n",
      "\n",
      "I'm doing well, thank you. How about you?\n",
      "\n",
      "\n",
      "I'm doing well, thank you. How about you?\n",
      "\n",
      "\n",
      "I'm doing well, thank you. How about you?\n",
      "\n",
      "\n",
      "I'm doing well, thank you. How about you?\n",
      "\n",
      "\n",
      "I'm good, thanks! How about you?\n",
      "\n",
      "\n",
      "I'm doing well. How about you?\n",
      "\n",
      "\n",
      "I'm doing great. How about you?\n",
      "\n",
      "\n",
      "I'm doing well, thank you. How about you?\n",
      "\u001B[1mSerial executed in 12.38 seconds.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "# 记录当前时间点\n",
    "s = time.perf_counter()\n",
    "# 使用同步方式串行执行生成文本的任务\n",
    "generate_serially()\n",
    "# 计算串行执行所花费的时间\n",
    "elapsed = time.perf_counter() - s\n",
    "print(\"\\033[1m\" + f\"Serial executed in {elapsed:0.2f} seconds.\" + \"\\033[0m\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-22T03:55:39.894316100Z",
     "start_time": "2023-08-22T03:55:27.513964Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.2 定制大语言模型\n",
    "如果您想使用自己的 LLM 或与 LangChain 中支持的包装器不同的包装器。\n",
    "自定义LLM只需要实现一件必需的事情：\n",
    "\n",
    "一个 _call 方法，它接受一个字符串、一些可选的非索引字，并返回一个字符串\n",
    "\n",
    "它可以实现第二个可选的东西：\n",
    "\n",
    "用于帮助打印此类 _identifying_params 的属性。应该返回字典。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# 让我们实现一个非常简单的自定义 LLM，它只返回输入的前 N 个字符。\n",
    "from typing import Any, List, Mapping, Optional\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "from langchain.llms.base import LLM\n",
    "\n",
    "#这个类 CustomLLM 继承了 LLM 类，并增加了一个新的类变量 n。\n",
    "#有两个 property 装饰的方法，分别是 _llm_type 和 _identifying_params，这两个方法都返回一些固定的属性值。\n",
    "#_call 方法主要是对输入的 prompt 字符串进行处理，返回前 n 个字符。如果提供了 stop 参数，它将引发一个异常。\n",
    "\n",
    "\n",
    "# 继承自 LLM 的 CustomLLM 类\n",
    "class CustomLLM1(LLM):\n",
    "\n",
    "    # 类变量，表示一个整数\n",
    "    n: int\n",
    "\n",
    "    # 一个属性装饰器，用于获取 _llm_type 的值\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        # 返回 \"custom\" 字符串作为 _llm_type 的值\n",
    "        return \"custom\"\n",
    "\n",
    "    # _call 方法用于处理某些操作\n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,  # 输入的提示字符串\n",
    "        stop: Optional[List[str]] = None,  # 可选的停止字符串列表，默认为 None\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,  # 可选的回调管理器，默认为 None\n",
    "    ) -> str:\n",
    "        # 如果 stop 参数不为 None，则抛出 ValueError 异常\n",
    "        if stop is not None:\n",
    "            raise ValueError(\"stop kwargs are not permitted.\")\n",
    "        # 返回 prompt 字符串的前 n 个字符\n",
    "        return prompt[: self.n]\n",
    "\n",
    "    # 一个属性装饰器，用于获取 _identifying_params 的值\n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        # 这个方法的文档字符串，说明这个方法的功能是获取标识参数\n",
    "        \"\"\"Get the identifying parameters.\"\"\"\n",
    "        # 返回一个字典，包含 n 的值\n",
    "        return {\"n\": self.n}\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-22T06:12:48.312119300Z",
     "start_time": "2023-08-22T06:12:48.274115500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "llm = CustomLLM1(n=10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-22T06:12:51.568591800Z",
     "start_time": "2023-08-22T06:12:51.554553700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "'This is a '"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"This is a foobar thing\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-22T06:12:52.150794600Z",
     "start_time": "2023-08-22T06:12:52.136786400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1mCustomLLM1\u001B[0m\n",
      "Params: {'n': 10}\n"
     ]
    }
   ],
   "source": [
    "print(llm)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-22T06:12:52.666683700Z",
     "start_time": "2023-08-22T06:12:52.620142500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
